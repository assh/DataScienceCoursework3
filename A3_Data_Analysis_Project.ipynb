{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## COMP5721M: Programming for Data Science\n",
    "\n",
    " ## Coursework 3: Data Analysis Project\n",
    "\n",
    " Last modified: 3 December 2023\n",
    "\n",
    " # Analysis Of Youtube Trending Data\n",
    "\n",
    "\n",
    " _Give names and emails of group members here:_\n",
    "\n",
    " * Asish Panda, mm23ap@leeds.ac.uk\n",
    " * Mohamed Imthiyas Abdul Rasheeth, mm23m2ia@leeds.ac.uk\n",
    " * Naveen Sabarinath Babu, mm23nsb@leeds.ac.uk\n",
    " * Roshan ., mm23rt@leeds.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project Plan\n",
    "\n",
    " ## The Data (10 marks)\n",
    "\n",
    " YouTube provides an API that provides information about trending data country-wise. The dataset on Kaggle is a real-time (daily) updating dataset derived from the API consisting of attributes for various countries, which is used for analysis in this project. This project's scope limited our use to a specific date range and limited the country to Great Britain.\n",
    "\n",
    " The dataset consists of two files, _GB_category_id.json and GB_youtube_trending_data.csv_. The files contain the following column ID's\n",
    "\n",
    " ### CSV:\n",
    "\n",
    " The CSV file is approximately 336MB in size and contains 16 columns and 238391 rows of data describing the videos properties.\n",
    "\n",
    "  ```\n",
    "  ['video_id',\n",
    "  'title',\n",
    "  'publishedAt',\n",
    "  'channelId',\n",
    "  'channelTitle',\n",
    "  'categoryId',\n",
    "  'trending_date',\n",
    "  'tags',\n",
    "  'view_count',\n",
    "  'likes',\n",
    "  'dislikes',\n",
    "  'comment_count',\n",
    "  'thumbnail_link',\n",
    "  'comments_disabled',\n",
    "  'ratings_disabled',\n",
    "  'description']\n",
    "  ```\n",
    "\n",
    "  \n",
    "\n",
    " ### JSON:\n",
    "\n",
    " The JSON file contains a data structure that links the categoryId column in each file. The JSON file also has information about each file category, i.e. ['family',' Entertainment',' Education']. The structure of the file is as follows. It is approximately 10 KB in size.\n",
    "\n",
    " ```\n",
    " {\n",
    "     \"kind\": \"youtube#videoCategoryListResponse\",\n",
    "     \"etag\": \"kBCr3I9kLHHU79W4Ip5196LDptI\",\n",
    "     \"items\": [\n",
    "         {\n",
    "             \"kind\": \"youtube#videoCategory\",\n",
    "             \"etag\": \"IfWa37JGcqZs-jZeAyFGkbeh6bc\",\n",
    "             \"id\": \"1\",\n",
    "             \"snippet\": {\n",
    "                 \"title\": \"{{category_string}}\",\n",
    "                 \"assignable\":{{boolean}},\n",
    "                 \"channelId\": \"{{string}}\"\n",
    "             }\n",
    "         },\n",
    "         {\n",
    "             \"kind\": \"youtube#videoCategory\",\n",
    "             \"etag\": \"5XGylIs7zkjHh5940dsT5862m1Y\",\n",
    "             \"id\": \"2\",\n",
    "             \"snippet\": {\n",
    "                 \"title\": \"{{category_string}}\",\n",
    "                 \"assignable\": {{boolean}},\n",
    "                 \"channelId\": \"{{string}}\"\n",
    "             }\n",
    "         },\n",
    " \t]\n",
    " }\n",
    " ```\n",
    "#### Reference for the Dataset\n",
    "\n",
    "Youtube. (2023). <i>YouTube Trending Video Dataset (updated daily)</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/7112357\n",
    "\n",
    " ## Project Aim and Objectives (5 marks)\n",
    "\n",
    " The primary objective of this project is to perform an in-depth analysis of the \"Trending Youtube API Dataset\" to identify and understand the essential attributes that result in the uploaded video being \"trending\" on the platform. The project aims to find the patterns in the dataset that would push the videos uploaded by the users into the trending category, thus helping the content creators optimize their video uploads accordingly, contributing to the YouTube algorithm.\n",
    "\n",
    " The aim of the project is achieved by using data analysis techniques and machine learning algorithms to find correlations between the different attributes in the dataset. We will use columns such as [categoryId, title,view_count, likes, dislikes,comment_count] to recognize the patterns resulting in a trending video. After a thorough analysis of the various aspects, we aim to discern the commonalities among the trending videos while also identifying the factors that may vary across the different genres of YouTube.\n",
    "\n",
    " In summary, the project aims to provide insight into the dynamics of YouTube trending videos, thereby helping content creators share content that would satisfy the YouTube algorithm and result in the video trending on the platform. The results will help in improving the experience of the users and the YouTube platform.\n",
    "\n",
    " ### Specific Objective(s)\n",
    "\n",
    " * __Objective 1:__ _Visualize the genres that trend in the United Kingdom_\n",
    " * __Objective 2:__ _Checks the key words to be used in the Title and/or Description for a video to be in the trending category_\n",
    " * __Objective 3:__ _Calculates whether the Likes, Dislikes and View Count influences a video to trend_\n",
    " * __Objective 4:__ _Implementing Regression Models to predict View Counts given Attributes_\n",
    "\n",
    " ## System Design (5 marks)\n",
    "\n",
    " ### Architecture\n",
    "\n",
    "The architecture implemented in this project has the following diagram.\n",
    "\n",
    "![alt text](architecture.jpg \"Architecture\")\n",
    "\n",
    " #### 1. Data Source\n",
    "\n",
    "The dataset is hosted on Kaggle and comprises two files. The .csv file contains data about each trending video, while the .json file contains information about the video category\n",
    " #### 2. Data Processing\n",
    "\n",
    "There are three steps performed while processing the data\n",
    "* The data is combined from the .csv and .json files into a single pandas data frame.\n",
    "* The data is scanned for duplicates and null values.  \n",
    "* Columns such as dates are expanded to provide finer details during analysis.\n",
    " #### 3. Data Analysis\n",
    "\n",
    "The data is analysed according to the first three objectives to determine the attributes of videos that affect their ability to trend on YouTube.\n",
    " #### 4. Visualisation\n",
    "\n",
    "The analysis results are visualised at each objective level to understand better the relationship between attributes and the ability of a video to trend on the platform.\n",
    " #### 5. Modelling\n",
    "\n",
    "Using information gained during analysis and visualisation, we proceed towards modelling our prediction algorithm.\n",
    "* Categorical variables are encoded using One Hot Encoding\n",
    "* Numerical variables are scaled such that they lie between 0 and 1\n",
    "* A test and train set is created for our regression model\n",
    "* The model is trained, and its predictions are verified using the test dataset\n",
    " \n",
    "\n",
    " ### Processing Modules and Algorithms\n",
    "\n",
    " _Briefly list and describe the most significant computational components of your system and the algorithms you will use to implement them.\n",
    " This could include things like:_\n",
    "\n",
    " * _Combination of data from .csv and .json dataset_\n",
    " * _WordCloud generation: A word cloud is generated using a library to visualise better any words that are frequently associated with the titles and tags of trending videos _\n",
    " * _Dummy Variables and Scaling: OneHotEncoding adds dummy variables to categorical data. MinMaxScaler is used to scale numerical data. Together, they are used to create correlation matrices to understand the relationship between dependent and independent variables._\n",
    " * _LinearRegression: sklearn's LinearRegression module is used to implement a baseline regression model_\n",
    " * _RandomForestRegression: sklearn's RandomForestRegression module is used to implement the final regression model that allows us to predict view counts given video attributes _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Program Code (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Module Imports\n",
    "\n",
    " We are importing ```pandas``` since it is used for DataFrame operations. Along with Pandas, we are also using the built-in ```JSON``` to import our ```GB_category_id.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Reading And Combining the two files\n",
    " In the following code block, we read the CSV as ```dfc``` and transform it into a pandas data frame called ```df```. We then read the JSON file into a variable called ```categories```. We then select the category title from ```categories``` and append it into a new column called ```category``` in the ```df``` data frame.\n",
    "\n",
    " We use the ```head()``` method to see how our data frame looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc= pd.read_csv('GB_youtube_trending_data.csv')\n",
    "df = pd.DataFrame(dfc)\n",
    "\n",
    "# Load the categories\n",
    "with open('GB_category_id.json') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "# Create a dictionary to map category IDs to category names\n",
    "category_dict = {int(item['id']): item['snippet']['title'] for item in categories['items']}\n",
    "\n",
    "# Map the category IDs in the dataframe to the category names\n",
    "df['category'] = df['categoryId'].map(category_dict)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We see that there are ```17``` features (columns) and ```238191``` trending videos (rows). To ensure correct analysis, we check if we do not have null items in any column by using isna(), which returns a boolean value and then sums it column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above output shows that ```4279``` missing descriptions and ```102``` missing categories. Since missing values in these columns will not cause problems in our analysis, we will not drop those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Duplicate Check\n",
    "\n",
    " We also check for duplicate rows in the data frame using the pandas' ```duplicated()``` function, as duplicate values could lead to bias. We then sum up the total to see how many rows have duplicates. From the result, we see that 124 rows have duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now use the ```drop_duplicates()``` function to drop those rows. We can then check the data frame's shape to cross-verify the deletion of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Check the important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Convert the ```trending_date``` column to datetime format and create new columns for year and month. The months are labeled to their names in English from their respective integer value to enhance clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['trending_date'])\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['month'].replace((1,2,3,4,5,6,7,8,9,10,11,12),('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'))\n",
    "df['month'] = pd.Categorical(df['month'], categories=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], ordered=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will convert the ```publishedAt``` column to Date-time datatype, since it is in object datatype currently. Then, we will take the published month and hour and append them in the two new columns called ```num_month``` and ```hour``` respectively. We have chosen ```publishedAt``` since the hour in trending videos were 00:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test_hour'] = df['date'].dt.hour\n",
    "df['test_hour'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "df['num_month'] = df['publishedAt'].dt.month\n",
    "df['hour'] = df['publishedAt'].dt.hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Objective 1 Code Cells: _Visualize the genres that trend in the United Kingdom_\n",
    "\n",
    " _Firstly, we will explore which category of video trends the most, to comprehend the kind of videos the viewers are more interested in viewing. We plot a ```barplot``` that shows the number of videos trending in each category._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar(title='Figure 1 - No. of Videos per Category',xlabel='Category',ylabel='Number of Trending Videos',color='royalblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see from Figure 1, Entertainment tops the list, followed by Sports, Gaming, Music, and People & Blogs as the Top 5 Categories across the dataset for the year between Aug 2020 and Nov 2023.\n",
    "\n",
    " _Now that we know the ranking of the trending categories, we further investigate the nature of the trending categories month-wise. A graph is plotted based on category and month to show the results of the investigation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot bar graph based on category and date by month\n",
    "df.groupby(['month','category']).size().unstack().plot(kind='bar',stacked=True,figsize=(18,12),title=\"Figure 2 - No. of Videos per Category by Month\",xlabel=\"Month\",ylabel='No. of Trending Videos').legend(bbox_to_anchor=(1.0, 1.0),title=\"Category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are using a stacked barplot to compare the number of trending videos each month. As shown in Figure 2, October has the highest number of trending videos, followed by September and August. The least number of trending videos is for February. The top 5 categories from Figure 1 have almost a similar spread across the plot.\n",
    "\n",
    " The spike in the number of trending videos between August and November could be because of the range of months of the trending videos in the dataset. That is, our range is between August 2020 to November 2023. We will further do analysis into finding how big of a difference is produced between each year by comparing only the top 5 categories.\n",
    "\n",
    " The data frame is further filtered to the Top 5 categories per Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot grouped bar graph for top 5 category per year\n",
    "df.groupby(['year','category']).size().groupby(level=0).nlargest(5).unstack().plot(kind='bar',stacked=False,figsize=(15,10),xlabel=\"Year\",ylabel='No. of Trending Videos',title=\"Figure 3 - Top 5 Categories per Year\").legend(bbox_to_anchor=(1.0, 1.0),title=\"Category\")\n",
    "plt.xticks(np.array([0,1,2,3]), ('2020','2021','2022','2023'),rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 3 shows that in 2020, there were fewer trending videos than in any other year. As for 2023, the number of trending videos is also less compared to 2021 and 2022. This shows that since we have incomplete data for 2020 and 2023, the height is reduced, which made it spike between August and November in Figure 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Objective 2 Code Cells:  _Checks the key words to be used in the Title and/or Description for a video to be in the trending category_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _To aide in our understanding of how keywords in title affect a videos abilty to trend, we use an external python module called ```worldcloud```. This will allow us to visualize frequently mentioned words in the title._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _We will calculate the total number of words in the title using the split() and len() functions and store them in a new column called title_length.Then, we take the mean value of title_length to see how many average words are there in each title._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the average length of title in words\n",
    "df_test3 = df.copy()\n",
    "df_test3['title_length'] = df_test3['title'].str.split().str.len()\n",
    "df_test3['title_length'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _We plot a histogram for ```title_length``` with 20 equal intervals.The data points are collected in the given interval, where the x-axis represents the number of words in the title and y-axis represents its count._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test3['title_length'].plot.hist(bins=20,figsize=(10,5),title=\"Figure 4 - Distribution of Title Length\",xlabel=\"Title Length\",ylabel=\"Frequency\",color='royalblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 4 shows that most of the videos that made it into the trending list has around 6 to 8 words. Thus, we could say that having either a very low or a very high word count in titles may not be appealing towards the viewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Now, we will create a word cloud to see the most used keywords for the title in the trending video for each category._\n",
    "\n",
    " _ _A text variable is created that takes the words in the ```title``` column based on categories and if the text is not empty, then generate the wordclouds for each category._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple graphs in the same cell\n",
    "fig, axes = plt.subplots(7, 2, figsize=(15, 30))\n",
    "\n",
    "# Get unique categories\n",
    "categories = df_test3['category'].unique()\n",
    "\n",
    "# Limit the number of categories to the number of subplots\n",
    "categories = categories[:len(axes.flatten())]\n",
    "\n",
    "# Plot the wordcloud for each category\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    category = categories[i]\n",
    "    title = df_test3[df_test3['category'] == category]['title']\n",
    "    text = ' '.join(title) \n",
    "    if text != ' ':\n",
    "        wordcloud = WordCloud(max_font_size=50, max_words=10000, background_color=\"white\").generate(text)\n",
    "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        ax.set_title(category,fontsize=20)\n",
    "        ax.axis(\"off\")\n",
    "        fig.suptitle('Figure 5 - Wordcloud of Trending Video Titles by Category',fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 5 shows the words that appears the most in each category. Considering the top 5 categories (from Objective 1), we can conclude that these words would increase the chances of the publishers video to make it into the trending list:\n",
    "\n",
    " * Entertainment : Ofiicial Trailer, Manchester United, Hot ones, Beta Squad, Official Teaser etc.\n",
    " * Sports : Premier League, Manchester United, League Highlights, Grand Prix, Man City etc.\n",
    " * Gaming : Minecraft, HARDCORE Minecraft, Minecraft Hardcore, Survived Days, Among Us etc.\n",
    " * Music : Official Music, Music Video, Offical Video, Lyric Video, Official Lyric etc.\n",
    " * People and Blogs : Official Video, Among Us, Short, Sidemen Among Us, Music Video etc.\n",
    "\n",
    " Words like ‘Slow Mo’ is of lesser significance since they are of a smaller font\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test4 = df.copy()\n",
    "df_test4['tag_length'] = df_test3['tags'].str.split().str.len()\n",
    "df_test4['tag_length'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  _Here, will create a word cloud to see the most used keywords for the tags in the trending video for each category._\n",
    "\n",
    "  _A text variable is created that takes the words in the ```tags``` column based on categories and if the text is not empty, then generate the wordclouds for each category._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple graphs in the same cell\n",
    "fig, axes = plt.subplots(7, 2, figsize=(15, 30))\n",
    "\n",
    "# Get unique categories\n",
    "categories = df_test3['category'].unique()\n",
    "\n",
    "# Limit the number of categories to the number of subplots\n",
    "categories = categories[:len(axes.flatten())]\n",
    "\n",
    "# Plot the wordcloud for each category\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    category = categories[i]\n",
    "    tags = df_test3[df_test3['category'] == category]['tags']\n",
    "    text = ' '.join(tags)\n",
    "    if text != ' ' or text == 'None':\n",
    "        wordcloud = WordCloud(max_font_size=50, max_words=10000, background_color=\"white\").generate(text)\n",
    "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        ax.set_title(category,fontsize=20)\n",
    "        ax.axis(\"off\")\n",
    "        fig.suptitle('Figure 6 - Wordcloud of Trending Video tags by Category',fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 6 shows the wordcloud for the most appearing words in tags, segregated by categories. Considering the top 5 categories (from Objective 1), we can conclude that these words would increase the chances of the publishers video to make it into the trending list:\n",
    "\n",
    " * Entertainment : Manchester United, Man Utd, United Man, Beta Squad, Transfer News  etc.\n",
    " * Sports : Sky Sport, Premier League, Manchester United, League football, Man Utd etc.\n",
    " * Gaming : Minecraft, battle royale, genshin impact, apex legends, Among Us etc.\n",
    " * Music : music video, hip hop, ed sheeran, taylor swift, lil durk etc.\n",
    " * People and Blogs : None, Sidemen, moreSidemen, calorie challenge, eating challenge etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figures 5, 6 and 7 explain the most used keywords based on their title and tag, where it has a common word in both wordclouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Next, we are creating a function ```wordcloud_gen_v2()``` that contains frequently used words in both attributes(tags and title)._\n",
    "\n",
    " _This function takes the tags and titles and segregates it based on categories. The ```common_words``` function finds the common words in both the texts variables, which is then ran through a for loop to generate a word cloud based on categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_gen_v2(category):\n",
    "\n",
    "    text1 = ' '.join(df_test3[df_test3['category'] == category]['title'])\n",
    "    text2 = ' '.join(df_test3[df_test3['category'] == category]['tags'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    # Generate word frequency dictionaries\n",
    "    word_freq1 = Counter(text1.split())\n",
    "    word_freq2 = Counter(text2.split())\n",
    "\n",
    "    # Find common words\n",
    "    common_words = word_freq1 & word_freq2\n",
    "\n",
    "    # Generate a word cloud using only the common words\n",
    "    common_text = ' '.join(set(common_words.elements()))\n",
    "    if common_text != ' ' or common_text == 'None':\n",
    "        wordcloud = WordCloud(max_font_size = 50,max_words=10000, background_color=\"white\").generate(common_text)\n",
    "    return wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _After grouping up the identical words of both attributes, We create a word cloud to show the similar words of trending videos by category._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple graphs in the same cell\n",
    "fig, axes = plt.subplots(7, 2, figsize=(15, 30))\n",
    "\n",
    "# Get unique categories\n",
    "categories = df_test3['category'].unique()\n",
    "\n",
    "# Limit the number of categories to the number of subplots\n",
    "categories = categories[:len(axes.flatten())]\n",
    "\n",
    "# Plot the wordcloud for each category\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    category = categories[i]\n",
    "    #tags = df_test3[df_test3['category'] == category]['tags']\n",
    "    #text = ' '.join(tags)\n",
    "    \n",
    "    if text != ' ' or text == 'None':\n",
    "        ax.imshow(wordcloud_gen_v2(category), interpolation=\"bilinear\")\n",
    "        ax.set_title(category,fontsize=20)\n",
    "        ax.axis(\"off\")\n",
    "        fig.suptitle('Figure 7 - Wordcloud of Common Words in Trending Video Tags and Title by Category',fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 7  shows common keywords in tags and titles for each category where the most repeated keywords are visualized as larger text and the least repeated as smaller text. We can conclude that these words would increase the chances of the publishers video to make it into the trending list if they are used in both titles and tags.\n",
    "\n",
    " * Entertainment : Challenge, Hour, Year, Youtuber, Official  etc.\n",
    " * Sports : Fight, Final, GAME, Transfer, team etc.\n",
    " * Gaming : Minecraft, Secret, World, Item, FIFA etc.\n",
    " * Music : Official, ft, feat, song, Album etc.\n",
    " * People and Blogs : Short, HOUSE, World, Dream, Videovlog etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Objective 3 Code Cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _We have to see all the columns but use only the ones we need for objective 3. Thus, we use the column method that returns the column names in the data frame._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Drop the unnecessary columns to explore the data in the ```df_ob3``` dataset; using the ```drop()``` method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3 = df.copy()\n",
    "df_ob3 = df.drop(['video_id','channelId','tags','channelTitle','thumbnail_link','comments_disabled','ratings_disabled','date','month','year','description'], axis=1)\n",
    "df_ob3.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now check the relation between likes, views, and comment count using scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['likes', 'view_count', 'comment_count']\n",
    "plt.figure(figsize=(16, 18))\n",
    "#suptitle and align title to center\n",
    "plt.suptitle('Figure 8 - Scatterplot of Likes, View Count and Comment Count',fontsize=20,x = .7, y = .95)\n",
    "#plt.suptitle('Figure 8 - Scatterplot of Likes, View Count and Comment Count',fontsize=20)\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i+1, len(columns)):\n",
    "        plt.subplot(len(columns), len(columns), i*len(columns) + j + 1)\n",
    "        sns.scatterplot(x=np.log(df[columns[i]]), y=np.log(df[columns[j]])).set(title=f'Scatterplot of {columns[i].title()} vs {columns[j].title()}', xlabel=columns[i].title(), ylabel=columns[j].title())\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Figure 8, we see that likes, view count, and comment count have a linear relationship, which means if the likes increase, the view count increases, and then the comment count also increases.\n",
    "\n",
    " _We then look deeper into likes and view counts for each category and look for the similarity. However, we also dropped the null rows._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['likes', 'view_count', 'comment_count']\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.suptitle('Figure 9 - Boxplot of Likes, View Count and Comment Count', fontsize=12,position=(0.4,1.05))\n",
    "plt.subplots_adjust(top=0.85)\n",
    "for i, column in enumerate(columns):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    sns.boxplot(x=np.log(df[column]))\n",
    "    plt.title(column.title())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plotting the boxplot with the actual count of likes, view_count and comment_count did not provide a graph that was convenient to read. Therefore, we have opted to take the logarithm of each values in the respective columns to better visualise it, which can be seen in Fig 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Relation between view count and category_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3 = df_ob3.dropna()\n",
    "\n",
    "plt.scatter(df_ob3['view_count'],df_ob3['category'].astype(str))\n",
    "plt.xlabel('View Count [in 100 million]')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Figure 10 - View count vs. Category')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 10 shows the view count in millions for each category, where Music has the highest number of views, followed by Entertainment and Education.\n",
    "\n",
    " _Relation between likes and category_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_ob3['likes'],df_ob3['category'].astype(str))\n",
    "plt.xlabel('Likes [in 100 million]')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Figure 11 - Likes vs. Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 11 shows the likes in millions for each category, where Music has the most likes, followed by Entertainment and Education.\n",
    "\n",
    " These two graphs show similar results, so looking from the perspective of view_count alone provides similar results. We chose view count since the values are higher than that of likes.\n",
    "\n",
    " _We then count the number of videos that are higher than the mean of the view count._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['title'][df_ob3['view_count'] > df_ob3['view_count'].mean()].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['title'][df_ob3['likes'] > df_ob3['likes'].mean()].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['title'][df_ob3['comment_count'] > df_ob3['comment_count'].mean()].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The mean of the number of view counts is higher than the number of likes and the number of comments in the dataset. Since we have shown that all three would provide almost similar results, we have chosen to go with view_count column alone for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['title'][df_ob3['view_count'] == 0].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we see 94 videos have made it to the trending dataset with 0 view count, so these can be considered as anomalies. From graphs, higher the view count -> more chances of it being recommended to users in that particular category than the ones with lower view count.\n",
    "\n",
    " _Three new columns have been added for visualisation purpose - ```view_count_mean```,```view_count50``` and ```view_count_25```._\n",
    "\n",
    " _view_count_mean - returns 1 if the view_count is greater than the mean of view_count_\n",
    "\n",
    " _view_count_50 - returns 1 if view_count is greater than the .5 quantile or 50th percentile of the data_\n",
    " \n",
    " _view_count_25 - returns 1 if view_count is greater than the .25 quantile or 50th percentile of the data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['view_count_mean'] = (df_ob3['view_count'] > df_ob3['view_count'].mean()).astype(int)\n",
    "df_ob3['view_count_50'] = (df_ob3['view_count'] > df_ob3['view_count'].quantile(.5)).astype(int)\n",
    "df_ob3['view_count_25'] = (df_ob3['view_count'] > df_ob3['view_count'].quantile(.25)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['view_count'].quantile(.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['view_count'].quantile(.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3['view_count'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3.groupby(['view_count_mean','category']).size().unstack().plot(kind='bar',stacked=False,figsize=(15,10),title=\"Figure 12 - View Count Mean vs. Category\",xlabel='View Count Mean',ylabel='No. of Trending Videos').legend(bbox_to_anchor=(1.0, 1.0),title=\"Category\").axes.set_xticklabels(['View Count Lesser than Mean', 'View Count Greater than Mean'],rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Figure 12, we see that the majority of the videos have a view count that is lesser than the mean. We can conclude that vidoes are not required to accumulate views that cross the mean of the view counts of the trending videos (2168369.3647721303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3.groupby(['view_count_50','category']).size().unstack().plot(kind='bar',stacked=False,figsize=(15,10),title=\"Figure 13 - View Count 50% vs. Category\",xlabel='View Count 50%',ylabel='No. of Trending Videos').legend(bbox_to_anchor=(1.0, 1.0),title=\"Category\").axes.set_xticklabels(['View Count Lesser than 50%', 'View Count Greater than 50%'],rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 13 shows the 50 percentile in view count. Some have the same number of videos on both sides, while more trending Sports videos are less than the 50 percentile, and more Gaming, Entertainment, and Music videos are above the 50 percentile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob3.groupby(['view_count_25','category']).size().unstack().plot(kind='bar',stacked=False,figsize=(15,10),title=\"Figure 14 - View Count 25% vs. Category\",xlabel='View Count 25%',ylabel='No. of Trending Videos').legend(bbox_to_anchor=(1.0, 1.0),title=\"Category\").axes.set_xticklabels(['View Count Lesser than 25%', 'View Count Greater than 25%'],rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above three graphs - Figures 12,13 and 14; gives us insights on the number of views a video has accumulated in the trending dataset. We can conclude that if a video successfully acquires views between the 25th percentile (358566.0 views) and 50th percentile (781394.0 views), then the video has a higher probability to make it into the Youtube Trending Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _We have not used dislikes for our analysis since Youtube's API has stopped registering dislikes count from 2022. We will plot a barplot to visualize this statistics._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from 'date' column\n",
    "df_test = df.copy()\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "df_test['dislikes_nonzero'] = df['dislikes'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Group by year and calculate sum of 'dislikes_zero'\n",
    "yearly_dislikes_nonzero = df_test.groupby('year')['dislikes_nonzero'].sum()\n",
    "\n",
    "# Plot bar graph\n",
    "yearly_dislikes_nonzero.plot(kind='bar', figsize=(15,10), title=\"Figure 15 - Yearly Dislikes\", xlabel='Year', ylabel='Sum of Dislikes', color='royalblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, from Figure 15, we can see there are dislikes in 2020 and 2021 alone. The empty bins for 2022 and 2023 suggest no data for dislikes count. The reason behind the dislikes count for 2020 being less than 2021 is that we need the complete data for 2020 (From January to December).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _```corr_list``` is a variable that includes all the columns that is to be checked with the correlation matrix._\n",
    "\n",
    " _Correlation means that there is a relationship between two things, but does not always mean that one causes the other. It ranges from -1 to 1, where 0 denotes 'No correlation' and 1 denotes positively correlated (If one variable increases, the other variable also increases), while -1 denotes negatively correlated (If one variable increases, the other variable decreases). We aim to see how close it is to -1 and 1 for correlation, the closer the more correlated._\n",
    "\n",
    " _We use the ```corr()``` method to get the correlation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list = ['hour','date','num_month','categoryId','year','likes','dislikes','comment_count','view_count']\n",
    "df[corr_list].corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Now that we have the correlation, we will use seaborn's feature called ```heatmap()``` to better visualize the correlation among them._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "fig = sns.heatmap(data = df[corr_list].corr(), annot=True).set_title(\"Figure 16 - Heatmap of Correlation Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From Figure 16, we see that the likes and view count is high-positively correlated with 0.83 value. The dislikes and view count, the comment count and view count, and the dislikes and comment count are also correlated to a certain extent. The rest of the variables are neglibly correlated as the value is closer to 0.\n",
    "\n",
    " We can see a very high positive correlation between year and date, and further explore on it by comparing it with likes, comment count and view count. We will use the ```lineplot``` method to plot the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 3, figsize=(25, 20))\n",
    "variables = ['likes', 'view_count', 'comment_count']\n",
    "x_values = ['hour','month']\n",
    "plt.suptitle('Figure 17 - Lineplot of Likes, View Count and Comment Count vs Hour and Month',fontsize=20)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "for i, x in enumerate(x_values):\n",
    "    for j, var in enumerate(variables):\n",
    "        sns.lineplot(x = df[x], y = df[var], data = df, ax = ax[i,j], marker = 'o').set_title(f'{var.title()} vs {x.title()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Figure 17 shows us the line plot for the number of likes, view count and comment count hourwise with respect to hours and month. From the figure 17, we see the majority of likes, view counts and comments are accrued for the videos pubished between 2 am and 7 am. We can also see the same trend for the videos published in the months between May and September.\n",
    "\n",
    " Although we have a time frame to see a spike of the video, we will now use the ```boxplot()``` to better visualize the spike hourly and monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 3, figsize=(25, 20))\n",
    "variables = ['likes', 'view_count', 'comment_count']\n",
    "x_values = ['hour','month']\n",
    "plt.suptitle('Figure 18 - Boxplot of Likes, View Count and Comment Count', fontsize= 20 ,position=(0.5,0.95))\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "for i, x in enumerate(x_values):\n",
    "    for j, var in enumerate(variables):\n",
    "        sns.boxplot(x = df[x], y = np.log(df[var]), data = df, ax = ax[i,j]).set_title(f'{var.title()} vs {x.title()}')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From Figure 18, we can now see the exact hour and month better. Thus, as per the graph, 5am is the best time and June is the best month to publish the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Objective 4 : _Implementing Regression Models to predict View Counts given Attributes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Importing Regression models and creating Test/Train Datasets.\n",
    "\n",
    " To begin with our analysis, we import the sci-kit-learn Python library. Scikit-learn contains a variety of practical classes that can help us with regression modelling. We import train_test_split, LinearRegression and RandomForestRegressor for modelling. We also import  mean_squared_error and r2_score to help us in evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform ```one hot encoding``` to create dummy variables for each categorical column, in order to produce a correlation matrix.\n",
    "\n",
    " Use a ```for loop``` to create a ```heatmap``` for each of the produced correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = df.copy()\n",
    "#list of columns to be one hot encoded\n",
    "ohe_columns = ['categoryId','num_month','hour','year']\n",
    "#one hot encode the columns\n",
    "df_ohe = pd.get_dummies(df_ohe, columns=ohe_columns)\n",
    "\n",
    "# drop columns that are not needed - title, publishedAt, trending_date, date, month, year, description, video_id, channelId, tags, channelTitle, thumbnail_link, comments_disabled, ratings_disabled, comment_count, likes, dislikes, view_count, category, likes_mean, likes_50, view_count_mean, view_count_50, view_count_25, view_to_like_ratio, like_to_view_ratio, dislikes_zero, dislikes_nonzero\n",
    "df_ohe = df_ohe.drop(['title','publishedAt','trending_date','date','month','description','video_id','channelId','tags','channelTitle','thumbnail_link','comments_disabled','ratings_disabled','category'], axis=1)\n",
    "\n",
    "# scale the numerical columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_ohe[['likes', 'dislikes', 'comment_count']] = scaler.fit_transform(df_ohe[['likes', 'dislikes', 'comment_count']])\n",
    "df_ohe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['categoryId','num_month','hour','year']:\n",
    "    df_corr = df[['view_count',col]]\n",
    "    df_ohe1 = pd.get_dummies(df_corr, columns= [col])\n",
    "    corr = df_ohe1.corr()\n",
    "    \n",
    "\n",
    "    f, ax = plt.subplots(figsize=(20, 20))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=1.5, ax=ax)\n",
    "\n",
    "    plt.title(f\"Figure 19 - Heatmap between number of views and {col}\", fontsize=21)\n",
    "\n",
    "    f.text(0.5, 0.04, 'View Count', ha='center', fontsize=18)\n",
    "    f.text(0.04, 0.5, f'{col}', va='center', rotation='vertical', fontsize=18)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Encoding Categorical Variables and Scaling\n",
    "\n",
    " Since our dataset contains categorical data like ```categoryId```, ```Month```, ```Hour``` and ```year```, we use OneHotEncoding function to create dummy variables.\n",
    "\n",
    " Similarly, our dataset contains numerical values in ```likes```, ```dislikes``` and ```comment_count``` which have varying numerical scale. We can thus use ```MinMaxScaler``` from Scikit-Learn to scale our numerical columns. Scikit-Learn alos contains ```StandardScaler```, but we are using ```MinMaxScaler``` since it scales the data from ```[0,1]``` and preserves the shape of the dataset.\n",
    "\n",
    " Finally, we drop all unnecessary columns in our dataframe to reduce computation complexity and then create a 20:80 test/train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = df.copy()\n",
    "#list of columns to be one hot encoded\n",
    "ohe_columns = ['categoryId','num_month','hour','year']\n",
    "#one hot encode the columns\n",
    "df_ohe = pd.get_dummies(df_ohe, columns=ohe_columns)\n",
    "\n",
    "# drop columns that are not needed - title, publishedAt, trending_date, date, month, year, description, video_id, channelId, tags, channelTitle, thumbnail_link, comments_disabled, ratings_disabled, comment_count, likes, dislikes, view_count, category, likes_mean, likes_50, view_count_mean, view_count_50, view_count_25, view_to_like_ratio, like_to_view_ratio, dislikes_zero, dislikes_nonzero\n",
    "df_ohe = df_ohe.drop(['title','publishedAt','trending_date','date','month','description','video_id','channelId','tags','channelTitle','thumbnail_link','comments_disabled','ratings_disabled','category'], axis=1)\n",
    "\n",
    "# scale the numerical columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_ohe[['likes', 'dislikes', 'comment_count']] = scaler.fit_transform(df_ohe[['likes', 'dislikes', 'comment_count']])\n",
    "df_ohe.head()\n",
    "\n",
    "X = df_ohe.drop(['view_count'], axis=1)\n",
    "Y = df_ohe['view_count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Creating and evaluating a LinearRegression model\n",
    "\n",
    " Secondly, we instantiate a linear regression model from the Scikit-Learn library and assign it to a variable called model. The object's fit method is used to train and create a linear regression model using our training dataset X_train. Finally, using the created model, we use our testing dataset X_test to predict our view count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predict the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluating the model\n",
    "\n",
    " We can then print the Mean Squared Error, R**2 and Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "print('Root Mean squared error: %.2f' % mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the output above, we get a variance score of 0.72. While this score is close to 1, we can try another model to ensure we get the best possible score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plotting Predicted vs Actual View Count For Linear Regression\n",
    "\n",
    " When we plot the graph for Linear Regression, we can observe that while Linear Regression can predict view counts for trending videos up to 1e8 views, it fails to accurately predict higher view counts, thus lowering our variance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual view count\")\n",
    "plt.ylabel(\"Predicted view count\")\n",
    "plt.title(\"Figure 23 - Actual vs Predicted view count\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,c = \"r\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using Random Forest Regressor\n",
    "\n",
    " We can now use Linear Regression as our baseline model to evaluate our next model. We are using RandomForestRegressor from Scikit-Learn to implement our model.\n",
    "\n",
    " RandomForestRegressor needs an argument called n_estimtor that determines the number of decision tree classifiers to use. To check the appropriate integer for the n_estimator, we can create a function that takes an n as the number estimator. The function returns MSE, R2 and RMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestList(n):\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=201750985)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can then pass the function through a for loop that iterates over the list of estimators as defined below. We can then append the resultant scores against the n_estimators to a new data frame.\n",
    "\n",
    " _Note: The below code to find the best n_estimator has been commented out after execution in order to ensure the Jupyter Notebook completes execution in a reasonable amount of time. The only functionality of this code is to find best n_estimator and is not supposed to be executed everytime._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame(columns=['n_estimators', 'MSE', 'RMSE', 'R2'])\n",
    "n_estimates = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50,55 ,60 ,65 ,70, 75 ,80 ,85 ,90 ,95 ,100]\n",
    "for i in n_estimates:\n",
    "    mse,r2,rmse = RandomForestList(i)\n",
    "    rf_df = pd.concat([rf_df, pd.DataFrame([{'n_estimators': i, 'MSE': mse, 'RMSE': rmse, 'R2': r2}])], ignore_index=True)\n",
    "    #rf_df = rf_df.append(, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Selection of Estimator value\n",
    "\n",
    " We can now plot RMSE vs n_estimator and R2 vs n_estimator. From the graph, we see that an increase in n_estimnator initially causes a rapid rise in R2 value, but only a marginal increase from n_estimator greater than 15. Furthermore, any increase in the value of n_estimnator does not cause any change in the value of R2. Thus, we decide that the value of n_estimator should be 50 to maximise our R2 while also minimising compute time.\n",
    "\n",
    " _Note: The below code to plot the best n_estimator has been commented out after execution in order to ensure the Jupyter Notebook completes execution in a reasonable amount of time. The only functionality of this code is to plot the best n_estimator and is not supposed to be executed everytime._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].plot(rf_df['n_estimators'], rf_df['RMSE'])\n",
    "axes[0].set_xlabel('n_estimators')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('Figure 24 - RMSE vs n_estimators')\n",
    "axes[0].axvline(x=50, color='r', linestyle='--')\n",
    "axes[1].plot(rf_df['n_estimators'], rf_df['R2'], color='orange')\n",
    "axes[1].set_xlabel('n_estimators')\n",
    "axes[1].set_ylabel('R2')\n",
    "axes[1].set_title('Figure 25 - R2 vs n_estimators')\n",
    "axes[1].axvline(x=50, color='r', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Training RandomForest Modedel using Selected n_estimator\n",
    "\n",
    " After determining 50 as the ideal value of n_estimator, we can start training our Random Forest Model. AFter training with X_train and y_train, we can test our model with the X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators = 50, random_state = 201750985)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "# predict the result\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluating the Model\n",
    "\n",
    " Printing the scores for our model shows a significant increase in the R2 value from 0.72 to 0.95. Thus, we can choose the RandomForestRegressor with n_estimator as 50 for our prediction Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "print('Root Mean squared error: %.2f' % mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plotting the graph of actual vs prediction shows our model has improved over just Linear Regression in determining View_count of Videos given a set of attributes about the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual view count\")\n",
    "plt.ylabel(\"Predicted view count\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,c = \"r\")\n",
    "plt.title(\"Figure 26: Actual vs Predicted view count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project Outcome (10 + 10 marks)\n",
    "\n",
    " _This section should describe the outcome of the project by means of both explanation of the results and by graphical visualisation in the form of graphs, charts or or other kinds of diagram_\n",
    "\n",
    " _The section should begin with a general overview of the results and then have a section for each of the project objectives. For each of these objectives an explanation of more specific results relating to that objective shoud be given, followed by a section presenting some visualisation of the results obtained. (In the case where\n",
    " the project had just one objective, you should still have a section describing\n",
    " the results from a general perspective followed by a section that focuses on\n",
    " the particular objective.)_\n",
    "\n",
    " _The marks for this section will be divided into 10 marks for Explanation\n",
    " and 10 marks for Visualisation. These marks will be awarded for the Project Outcome\n",
    " section as a whole, not for each objective individually. Hence, you do not\n",
    " have to pay equal attention to each. However, you are expected to have a\n",
    " some explanation and visualisation for each. It is suggested you have\n",
    " 200-400 words explanation for each objective._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Overview of Results\n",
    " _Give a general overview of the results (around 200 words)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Objective 1\n",
    "We intended to find how each category trends and find out the top trending categories as well. So, we used the entire dataset to find its popularity for each category, and then we further investigated the trend of the categories for each month. Finally, we confirmed our findings by studying only the top 5 categories yearly. \n",
    "\n",
    "We used the stacked and unstacked barplot to better understand the data. Below are our main findings:\n",
    "\n",
    "* The Entertainment category has the most trending videos from August 2020 to November 2023, followed by Sports, Gaming, Music, and People & Blogs. We also noticed that the rest of the categories are fivefold less than the Entertainment Category. The Travel & Events and the Pets & Animals categories have the least trending videos.\n",
    "\n",
    "* We also see a spike in the number of trending videos between August and November compared to other months in the graph. This is because our data range is between August 2020 and November 2023, which biased August, September, October, and November to have unusually more significant lengths than the rest. This assumption is proved in (Figure 3) bar plots across four years, where in 2020, the number of bins in the plot is comparatively smaller than that of other bins. 2023 is also smaller than 2021 and 2022, as we need data for December 2023. \n",
    "\n",
    "* We can also see that the top five categories have the most spread in the stacked (Figure 2) barplot, which means that the top five categories are the same for each month. \n",
    "We also noticed a change in trend across 2021 to 2023, where the videos in the gaming category are more trending, while the Entertainment, People & Blogs categories are decreasing year by year. Sports and Music categories stay in almost similar trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Objective 2\n",
    "\n",
    " We wanted to see if the keywords in the title or tags impact the video trend. So, we used word cloud to pull up the most used keywords in the trending videos.\n",
    " We observed the following outcomes :\n",
    "\n",
    " * We noticed that, on average, ``nine ` words are used in the title. \n",
    "\n",
    " * We can see that the dark and big words in the word cloud are more used keywords and have the highest significance. Thus, in the Entertainment category (in Figure 5), keywords like ‘Official’, and ’Trailer’ are used more, while ‘Slow Mo’ is used less in the title. The same goes for the rest of the categories, where ‘Official Video’ tops the list across a few categories, followed by the local keywords such as ‘Prime Minister’ for News & Politics, ‘League Highlights’ for Sports, ‘iPhone Pro’ for Science & Technology in regards to the keywords used in the title.\n",
    "\n",
    " * The audience is more intrigued to watch videos with keywords relating to current affairs in almost every category. We verify this by seeing that keywords like 'Boris Johnson' and 'Prime Minister' are used more in the News & Politics category, which suggests that the keywords used from current affairs make the video trend.\n",
    " \n",
    " * We spot that “Season” is the most commonly used keyword in the title and the video tags in the Entertainment category (from Figure 7). The keywords used in the title are also used in the video tags, with which one could assume that the frequency of keywords used impacts the reach of the videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Objective 3\n",
    " The aim of objective 3 was to analyse if the number of likes, views, comments or dislikes had an impact on deciding whether an uploaded video makes it into the trending list or not. The following points conclude our findings on this topic.\n",
    "\n",
    " * We have concluded that dislikes cannot be used as a dependent factor, since Youtube API has discontinued the public display of likes from the year 2022, as shown in Figure 15. Thereby, leading us to continue our analysis using the other three variables.\n",
    "\n",
    " * From Figure 8, it is evident that the number of likes, comments and views are almost synonymous, i.e., performing data analysis on these three factors would produce almost similar results. This statement is backed by Figures 9 and 10. Furthermore, the number of views were used for further analysis.\n",
    "\n",
    " * Figures 12,13 and 14; give us insights on the number of views a video needs to accumulate in order to enter into the Youtube Trending Charts. We can conclude that if a video successfully acquires views between the 25th percentile (358566.0 views) and 50th percentile (781394.0 views), it increases the probability for the video to trend among the userbase.\n",
    "\n",
    " * Figure 16 shows the results of a heatmap that lets us know that likes, views,comments,dislikes, year and hour some of the positive significant factors for a video to trend. Using these factors, a lineplot and boxplot were plotted (as seen in Figure 17 and 18) which declared that most of the trending videos were uploaded in the months of June to September, in the early hours of 4AM to 6AM. This analysis could state the best possible times to upload a video in order to generate the maximum number of views,likes and comments (user interaction).\n",
    "\n",
    " Thus, the user interaction in a video does play a crucial role in deciding whether a video makes it into the trending charts or not, so the higher the user interaction, more is the probability to trend. The factors that increase the odds of user interaction have also been found and analysed.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Objective 4\n",
    "\n",
    "We aimed to predict the number of views based on dependent variables, such as likes, comment_count, categoryId and published date. We have implemented Linear Regression as a baseline model and a Random Forest Algorithm to create a prediction model. \n",
    "\n",
    "We have used MSE (Mean Squared Error), RMSE (Root Mean Squared Error) and R2 (Variance) to evaluate the prediction accuracy of the model, and the following results were observed.\n",
    "\n",
    "Linear Regression\n",
    "- The MSE score is 9738287750508.69\n",
    "- The RMSE score is 3120622.97\n",
    "- The R2 value is 0.72\n",
    "From the above scores, we can assert that our model does provide a satisfactory result but can be further improved by utilising Random Forest Regression. Figure 23 should show all the points on the dotted diagonal line, but our Linear Regression model shows a considerable diversion as our actual view count increases.\n",
    "\n",
    "Random Forest Regression\n",
    "\n",
    "Random Forest Regression uses a n_estimator, which is the number of decision tree classifiers to use. To determine the appropriate n_estimator, we have iterated over a range from 1 to 100. The results of the iteration have been plotted in ```figure 24``` and ```figure 25```. The results show that while there is a considerable rise in variance for lesser values of n, the variance flatlines after n  = 50. Thus, we choose 50 as our n_estimator.\n",
    "\n",
    "Using 50 as our n_estimator for our model gives us the following results\n",
    "\n",
    "- The MSE score is 1859860906199.00\n",
    "- The RMSE score is 1363767.17\n",
    "- The R2 value is 0.95\n",
    "\n",
    "From the above scores, it is clear that Random Forest Regression is better at estimating the number of views in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conclusion (5 marks)\n",
    "\n",
    " ### Achievements\n",
    "\n",
    " The principal factors regarding the Youtube Trending Charts have been thoroughly analysed and we have concluded that the number of views is the factor that plays a major role in deciding the chances of a video making it into the trending charts, followed by the category of the video and the words used in the title. We have therefore implemented algorithms to devise a model that predicts the number of views that a video would generate based on these dependent factors, and have been successful in predicting the output with an accuracy score of 95%. \n",
    "\n",
    " ### Limitations\n",
    "The limitations include the following :\n",
    "\n",
    "1. Insufficient Data: As we only have data from August 2020 to November 2023, we can only see the trend for the recent years. If we had enough data for at least five years, we could have better predicted trends and accuracy.\n",
    "\n",
    "2. ZeroDislikes from 2022: YouTube has stopped publishing the dislikes count since 2022. If we had dislike counts for our entire dataset, we would have investigated to see if this factor has any impact on the trending of the videos, along with the likes, view counts, and comment counts.\n",
    "\n",
    "3. Time Complexity: Modelling RandomForest for large-scale datasets like YouTube's API takes a lot of time to compute. RandomForest is an ensemble model of Decision Trees. The time complexity for building a complete unpruned decision tree is O( v * n * log(n) ), where n is the number of records and v is the number of variables/attributes. In order to solve this problem, we can use VPC/VPS for better computational capability to make more complex models further.\n",
    "\n",
    "### Future Work\n",
    "For future works, we can enhance the project by further probing into the following :\n",
    "\n",
    "1. Add More Countries: The trends could be investigated for other countries such as the United States, India, Canada, France, etc., as well as the use of local languages in descriptions, titles, etc.\n",
    "\n",
    "2. Make it more user-configurable: The code could be made more dynamic where the user can compare two or more countries of his choice and also select which variables to use.\n",
    "\n",
    "3. Usage of different predition algorithms: Algorithms such as Classification, KNN, Neural Networks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# load\n",
    "with open('rf_df.pkl', 'rb') as f:\n",
    "    rf_df = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
